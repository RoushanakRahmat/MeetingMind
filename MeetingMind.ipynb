{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Install required packages if you run this in Colab or your local environment\n",
        "# !apt-get install -y poppler-utils\n",
        "# !pip install pdf2image google-cloud-vision\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from google.cloud import vision\n",
        "\n",
        "# Step 1: Upload your PDF file\n",
        "print(\"Upload your PDF file:\")\n",
        "uploaded_pdf = files.upload()\n",
        "pdf_path = list(uploaded_pdf.keys())[0]\n",
        "\n",
        "# Step 2: Upload your Google Cloud JSON key\n",
        "print(\"Upload your Google Cloud JSON key file:\")\n",
        "uploaded_json = files.upload()\n",
        "json_path = list(uploaded_json.keys())[0]\n",
        "\n",
        "# Step 3: Set environment variable for authentication\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = json_path\n",
        "\n",
        "# Step 4: Convert PDF pages to images and extract text using Vision API\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Convert PDF to images\n",
        "    pages = convert_from_path(pdf_path, dpi=300)\n",
        "\n",
        "    # Initialize Google Vision client\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    slide_texts = []\n",
        "    for i, page in enumerate(pages):\n",
        "        image_path = f'slide_{i+1}.jpg'\n",
        "        page.save(image_path, 'JPEG')\n",
        "\n",
        "        with open(image_path, 'rb') as image_file:\n",
        "            content = image_file.read()\n",
        "\n",
        "        image = vision.Image(content=content)\n",
        "        response = client.document_text_detection(image=image)\n",
        "\n",
        "        if response.error.message:\n",
        "            raise Exception(f\"API Error: {response.error.message}\")\n",
        "\n",
        "        text = response.full_text_annotation.text\n",
        "        slide_texts.append(text)\n",
        "\n",
        "    return slide_texts\n",
        "\n",
        "# Step 5: Run extraction and print results\n",
        "slides_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "for i, text in enumerate(slides_text):\n",
        "    print(f\"--- Slide {i+1} Text ---\")\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "xlcdjMaScc3D"
      },
      "id": "xlcdjMaScc3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "from google.cloud import speech_v1p1beta1 as speech\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Step 1: Upload your MP3 file\n",
        "print(\"Upload your audio file (MP3 format):\")\n",
        "uploaded_audio = files.upload()\n",
        "mp3_path = list(uploaded_audio.keys())[0]\n",
        "\n",
        "# Step 2: Upload your Google Cloud JSON key\n",
        "print(\"Upload your Google Cloud JSON key file:\")\n",
        "uploaded_json = files.upload()\n",
        "json_path = list(uploaded_json.keys())[0]\n",
        "\n",
        "# Step 3: Set environment variable\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = json_path\n",
        "\n",
        "# Step 4: Convert MP3 to WAV (mono, 16kHz)\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def convert_mp3_to_wav(mp3_path, wav_path=\"converted_audio.wav\"):\n",
        "    audio = AudioSegment.from_mp3(mp3_path)\n",
        "    audio = audio.set_channels(1)  # Force mono\n",
        "    audio = audio.set_frame_rate(16000)  # Force 16kHz sample rate\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "    return wav_path\n",
        "\n",
        "wav_path = convert_mp3_to_wav(mp3_path)\n",
        "print(f\"Audio converted to WAV: {wav_path}\")\n",
        "\n",
        "# Step 5: Transcribe with speaker diarization\n",
        "def transcribe_audio_with_speaker_diarization(audio_path):\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "        content = audio_file.read()\n",
        "\n",
        "    audio = speech.RecognitionAudio(content=content)\n",
        "\n",
        "    config = speech.RecognitionConfig(\n",
        "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "        sample_rate_hertz=16000,\n",
        "        language_code=\"en-US\",\n",
        "        enable_speaker_diarization=True,\n",
        "        diarization_speaker_count=2,  # Adjust as needed\n",
        "        enable_automatic_punctuation=True,\n",
        "        model=\"video\"\n",
        "    )\n",
        "\n",
        "    print(\"Transcribing audio... This may take a moment.\")\n",
        "    response = client.recognize(config=config, audio=audio)\n",
        "\n",
        "    if not response.results:\n",
        "        return \"No transcription result found.\"\n",
        "\n",
        "    result = response.results[-1]\n",
        "    words_info = result.alternatives[0].words\n",
        "\n",
        "    # Group words by speaker\n",
        "    transcript_by_speaker = {}\n",
        "    for word_info in words_info:\n",
        "        speaker_tag = word_info.speaker_tag\n",
        "        word = word_info.word\n",
        "        if speaker_tag not in transcript_by_speaker:\n",
        "            transcript_by_speaker[speaker_tag] = []\n",
        "        transcript_by_speaker[speaker_tag].append(word)\n",
        "\n",
        "    # Format transcript\n",
        "    transcript_text = \"\"\n",
        "    for speaker, words in transcript_by_speaker.items():\n",
        "        speaker_text = \" \".join(words)\n",
        "        transcript_text += f\"[Speaker {speaker}]: {speaker_text}\\n\\n\"\n",
        "\n",
        "    return transcript_text\n",
        "\n",
        "# Run transcription\n",
        "transcript = transcribe_audio_with_speaker_diarization(wav_path)\n",
        "print(\"\\n--- Transcription Result ---\\n\")\n",
        "print(transcript)\n"
      ],
      "metadata": {
        "id": "j01IaB8jf6Fs"
      },
      "id": "j01IaB8jf6Fs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import tempfile\n",
        "import subprocess\n",
        "from google.cloud import storage, speech\n",
        "from pydub import AudioSegment\n",
        "import fitz  # PyMuPDF for PDF text extraction\n",
        "\n",
        "# Google Cloud setup\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"service_account.json\"\n",
        "GCS_BUCKET = \"my-meeting\"\n",
        "\n",
        "storage_client = storage.Client()\n",
        "speech_client = speech.SpeechClient()\n",
        "\n",
        "# ======================== PDF Processing ========================\n",
        "def process_pdf(file):\n",
        "    try:\n",
        "        with fitz.open(file.name) as doc:\n",
        "            text = \"\\n\".join(page.get_text() for page in doc)\n",
        "        return text if text.strip() else \"No text detected in PDF.\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to process PDF: {str(e)}\"\n",
        "\n",
        "# ======================== Audio Processing ========================\n",
        "def preprocess_audio_to_wav(audio_file):\n",
        "    audio_path = audio_file.name\n",
        "    wav_path = tempfile.mktemp(suffix=\".wav\")\n",
        "\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio = audio.set_channels(1)\n",
        "    audio = audio.set_frame_rate(16000)\n",
        "    audio.export(wav_path, format=\"wav\")\n",
        "\n",
        "    return wav_path\n",
        "\n",
        "def transcribe_audio_file(audio_file):\n",
        "    try:\n",
        "        wav_path = preprocess_audio_to_wav(audio_file)\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET)\n",
        "        blob_name = f\"temp_audio/{os.path.basename(wav_path)}\"\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(wav_path)\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET}/{blob_name}\"\n",
        "\n",
        "        audio = speech.RecognitionAudio(uri=gcs_uri)\n",
        "        config = speech.RecognitionConfig(\n",
        "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "            sample_rate_hertz=16000,\n",
        "            language_code=\"en-GB\",\n",
        "            enable_automatic_punctuation=True\n",
        "        )\n",
        "\n",
        "        operation = speech_client.long_running_recognize(config=config, audio=audio)\n",
        "        response = operation.result(timeout=600)\n",
        "\n",
        "        transcript = \"\"\n",
        "        for result in response.results:\n",
        "            transcript += result.alternatives[0].transcript + \"\\n\"\n",
        "\n",
        "        blob.delete()\n",
        "\n",
        "        return transcript if transcript.strip() else \"No speech detected.\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to transcribe audio: {str(e)}\"\n",
        "\n",
        "# ======================== Video Processing ========================\n",
        "def extract_audio_with_ffmpeg(video_path):\n",
        "    wav_path = tempfile.mktemp(suffix=\".wav\")\n",
        "    cmd = f\"ffmpeg -i '{video_path}' -ac 1 -ar 16000 -vn -y '{wav_path}'\"\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "    return wav_path\n",
        "\n",
        "def transcribe_video_file(video_file):\n",
        "    try:\n",
        "        tmp_video_path = video_file.name\n",
        "\n",
        "        wav_path = extract_audio_with_ffmpeg(tmp_video_path)\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET)\n",
        "        blob_name = f\"temp_audio/{os.path.basename(wav_path)}\"\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(wav_path)\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET}/{blob_name}\"\n",
        "\n",
        "        audio = speech.RecognitionAudio(uri=gcs_uri)\n",
        "        config = speech.RecognitionConfig(\n",
        "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "            sample_rate_hertz=16000,\n",
        "            language_code=\"en-GB\",\n",
        "            enable_automatic_punctuation=True\n",
        "        )\n",
        "\n",
        "        operation = speech_client.long_running_recognize(config=config, audio=audio)\n",
        "        response = operation.result(timeout=600)\n",
        "\n",
        "        transcript = \"\"\n",
        "        for result in response.results:\n",
        "            transcript += result.alternatives[0].transcript + \"\\n\"\n",
        "\n",
        "        blob.delete()\n",
        "\n",
        "        return transcript if transcript.strip() else \"No speech detected.\"\n",
        "    except Exception as e:\n",
        "        return f\"Video transcription failed: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "3Mq9PztENGxN"
      },
      "id": "3Mq9PztENGxN",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai import init\n",
        "\n",
        "# Initialise Vertex AI\n",
        "init(project=\"ai-memory-rebuilder\", location=\"us-central1\")\n",
        "\n",
        "def summarise_text(text):\n",
        "    try:\n",
        "        if not text.strip():\n",
        "            return \"No text provided for summarisation.\"\n",
        "\n",
        "        # Load Gemini Pro model\n",
        "        model = GenerativeModel(\"gemini-1.0-pro\")\n",
        "\n",
        "        # Request summary\n",
        "        response = model.generate_content(\n",
        "            f\"Please summarise the following text in clear British English:\\n\\n{text}\"\n",
        "        )\n",
        "\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Summarisation failed: {str(e)}\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2spCwqS5MHwJ"
      },
      "id": "2spCwqS5MHwJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Multimodal Transcriber with Gemini Summarisation\")\n",
        "\n",
        "    with gr.Tab(\"PDF Transcription\"):\n",
        "        pdf_input = gr.File(file_types=[\".pdf\"])\n",
        "        pdf_output = gr.Textbox(label=\"Extracted Text\", lines=20)\n",
        "        pdf_summary = gr.Textbox(label=\"Summary\", lines=10)\n",
        "        pdf_button = gr.Button(\"Transcribe PDF\")\n",
        "        pdf_summarise_button = gr.Button(\"Summarise\")\n",
        "\n",
        "        pdf_button.click(fn=process_pdf, inputs=pdf_input, outputs=pdf_output)\n",
        "        pdf_summarise_button.click(fn=summarise_text, inputs=pdf_output, outputs=pdf_summary)\n",
        "\n",
        "    with gr.Tab(\"Audio Transcription\"):\n",
        "        audio_input = gr.File(file_types=[\".mp3\", \".wav\"])\n",
        "        audio_output = gr.Textbox(label=\"Transcribed Text\", lines=20)\n",
        "        audio_summary = gr.Textbox(label=\"Summary\", lines=10)\n",
        "        audio_button = gr.Button(\"Transcribe Audio\")\n",
        "        audio_summarise_button = gr.Button(\"Summarise\")\n",
        "\n",
        "        audio_button.click(fn=transcribe_audio_file, inputs=audio_input, outputs=audio_output)\n",
        "        audio_summarise_button.click(fn=summarise_text, inputs=audio_output, outputs=audio_summary)\n",
        "\n",
        "    with gr.Tab(\"Video Transcription\"):\n",
        "        video_input = gr.File(file_types=[\".mp4\", \".mov\"])\n",
        "        video_output = gr.Textbox(label=\"Transcribed Text\", lines=20)\n",
        "        video_summary = gr.Textbox(label=\"Summary\", lines=10)\n",
        "        video_button = gr.Button(\"Transcribe Video\")\n",
        "        video_summarise_button = gr.Button(\"Summarise\")\n",
        "\n",
        "        video_button.click(fn=transcribe_video_file, inputs=video_input, outputs=video_output)\n",
        "        video_summarise_button.click(fn=summarise_text, inputs=video_output, outputs=video_summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ehwCa0tyPCYe",
        "outputId": "30db1545-b62a-4572-8724-45ee8f84d1e0"
      },
      "id": "ehwCa0tyPCYe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2a4fc6a729ea522bce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2a4fc6a729ea522bce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "roushanakrahmat (Jun 29, 2025, 8:51:28â€¯PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}